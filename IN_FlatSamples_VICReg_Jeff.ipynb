{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeefce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports basics\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import setGPU\n",
    "import sklearn\n",
    "import corner\n",
    "import os\n",
    "import scipy\n",
    "# Imports neural net tools\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.variable import *\n",
    "import torch.optim as optim\n",
    "from fast_soft_sort.pytorch_ops import soft_rank\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbf55b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not enough arguments for format string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m weightCorr1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m#(most useful in barlow)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m weightCorr2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m#(not really useful but could explore)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m loss_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%i\u001b[39;49;00m\u001b[38;5;124;43mD encoder, $\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mlambda_\u001b[39;49m\u001b[38;5;132;43;01m{repr}\u001b[39;49;00m\u001b[38;5;124;43m$=\u001b[39;49m\u001b[38;5;132;43;01m%i\u001b[39;49;00m\u001b[38;5;124;43m, $\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mlambda_\u001b[39;49m\u001b[38;5;132;43;01m{cov}\u001b[39;49;00m\u001b[38;5;124;43m$=\u001b[39;49m\u001b[38;5;132;43;01m%i\u001b[39;49;00m\u001b[38;5;124;43m, $\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mlambda_\u001b[39;49m\u001b[38;5;132;43;01m{std}\u001b[39;49;00m\u001b[38;5;124;43m$=\u001b[39;49m\u001b[38;5;132;43;01m%i\u001b[39;49;00m\u001b[38;5;124;43m, $\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mlambda_\u001b[39;49m\u001b[38;5;132;43;01m{corr1}\u001b[39;49;00m\u001b[38;5;124;43m$=\u001b[39;49m\u001b[38;5;132;43;01m%i\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_encoded_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweightrepr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweightstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweightCorr1\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontrastiveVICreg_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_encoded_nodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_dimensions_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweightrepr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mRepr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweightstd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mCov_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweightstd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mStd_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweightCorr1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mCorr1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m outdir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/uscms_data/d3/jkrupa/flat/FlatSamples/plots/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mlabel\n",
      "\u001b[0;31mTypeError\u001b[0m: not enough arguments for format string"
     ]
    }
   ],
   "source": [
    "# Opens files and reads data\n",
    "\n",
    "print(\"Extracting\")\n",
    "n_encoded_nodes = 2\n",
    "weightrepr = 1\n",
    "weightcov = 100 #(most useful)\n",
    "weightstd = 1\n",
    "weightCorr1 = 0 #(most useful in barlow)\n",
    "weightCorr2 = 0 #(not really useful but could explore)\n",
    "\n",
    "loss_text=\"%iD encoder, $\\lambda_{repr}$=%i, $\\lambda_{cov}$=%i, $\\lambda_{std}$=%i, $\\lambda_{corr1}$=%i\"%(n_encoded_nodes, weightrepr, weightstd,weightCorr1 )\n",
    "\n",
    "label=f'contrastiveVICreg_{n_encoded_nodes}_dimensions_{weightrepr}Repr_{weightstd}Cov_{weightstd}Std_{weightCorr1}Corr1'\n",
    "outdir = '/uscms_data/d3/jkrupa/flat/FlatSamples/plots/'+label\n",
    "os.system(f'mkdir -p {outdir}')\n",
    "fOne = h5py.File(\"/uscms_data/d3/eamoreno/FlatSamples/data/FullQCD_FullSig_Zqq_noFill_dRlimit08_50particlesordered_genMatched50.h5\", 'r')\n",
    "totalData = fOne[\"deepDoubleQ\"][:]\n",
    "print(totalData.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378cd508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets controllable values\n",
    "\n",
    "particlesConsidered = 50\n",
    "particlesPostCut = 50\n",
    "entriesPerParticle = 4\n",
    "eventDataLength = 6\n",
    "decayTypeColumn = -1\n",
    "datapoints = 1400000\n",
    "#datapoints = len(totalData)\n",
    "trainingDataLength = int(len(totalData)*0.8)\n",
    "validationDataLength = int(len(totalData)*0.1)\n",
    "modelName = \"IN_FlatSamples_EighthQCDEighthSig_50particles_pTsdmassfilling_dRlimit08\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685318e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates Training Data\n",
    "\n",
    "print(\"Preparing Data\")\n",
    "\n",
    "particleDataLength = particlesConsidered * entriesPerParticle\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(totalData)\n",
    "\n",
    "#trainingDataLength = int(datapoints*0.8)\n",
    "#validationDataLength = int(datapoints*0.1)\n",
    "\n",
    "mask = [i>40 for i in totalData[:, eventDataLength-1]]\n",
    "totalData = totalData[mask]\n",
    "print(totalData)\n",
    "labels = totalData[:, decayTypeColumn:]\n",
    "particleData = totalData[:, eventDataLength:particleDataLength + eventDataLength]\n",
    "eventData = totalData[:, :eventDataLength]\n",
    "jetMassData = totalData[:, eventDataLength-1] #last entry in eventData (zero indexing)\n",
    "\n",
    "\n",
    "######### Training Data ###############\n",
    "eventTrainingData = np.array(eventData[0:trainingDataLength],dtype=np.float16)\n",
    "jetMassTrainingData = np.array(jetMassData[0:trainingDataLength],dtype=np.float16)\n",
    "particleTrainingData = np.transpose(\n",
    "    particleData[0:trainingDataLength, ].reshape(trainingDataLength, \n",
    "                                                 entriesPerParticle, \n",
    "                                                 particlesConsidered),\n",
    "                                                 axes=(0, 1, 2))\n",
    "particleTrainingData = particleTrainingData.astype(np.float16)\n",
    "\n",
    "trainingLabels = np.array([[i, 1-i] for i in labels[0:trainingDataLength]]).reshape((-1, 2))\n",
    "\n",
    "torch.save(torch.Tensor(particleTrainingData),f\"{outdir}/{label}_particleTrainingData.pt\")\n",
    "torch.save(torch.Tensor(jetMassTrainingData),f\"{outdir}/{label}_jetMassTrainingData.pt\")\n",
    "torch.save(torch.Tensor(trainingLabels),f\"{outdir}/{label}_trainingLabels.pt\")\n",
    "\n",
    "\n",
    "########## Validation Data ##########\n",
    "eventValidationData = np.array(eventData[trainingDataLength:trainingDataLength + validationDataLength])\n",
    "jetMassValidationData = np.array(jetMassData[trainingDataLength:trainingDataLength + validationDataLength])\n",
    "particleValidationData = np.transpose(\n",
    "    particleData[trainingDataLength:trainingDataLength + validationDataLength, ].reshape(validationDataLength,\n",
    "                                                                                         entriesPerParticle,\n",
    "                                                                                         particlesConsidered),\n",
    "                                                                                         axes=(0, 1, 2))\n",
    "validationLabels = np.array([[i, 1-i] for i in labels[trainingDataLength:trainingDataLength + validationDataLength]]).reshape((-1, 2))\n",
    "\n",
    "\n",
    "\n",
    "########### Testing Data ############\n",
    "particleTestData = np.transpose(particleData[trainingDataLength + validationDataLength:,].reshape(\n",
    "    len(particleData) - trainingDataLength - validationDataLength, entriesPerParticle, particlesConsidered),\n",
    "                                axes=(0, 1, 2))\n",
    "testLabels = np.array(labels[trainingDataLength + validationDataLength:])\n",
    "\n",
    "print('Selecting particlesPostCut')\n",
    "particleTrainingData = particleTrainingData[:, :particlesPostCut]\n",
    "particleValidationData = particleValidationData[:, :particlesPostCut]\n",
    "\n",
    "particlesConsidered = particlesPostCut\n",
    "\n",
    "# Separating signal and bkg arrays\n",
    "particleTrainingDataSig = particleTrainingData[trainingLabels[:,0].astype(bool)]\n",
    "particleTrainingDataBkg = particleTrainingData[trainingLabels[:,1].astype(bool)]\n",
    "particleValidationDataSig = particleValidationData[validationLabels[:,0].astype(bool)]\n",
    "particleValidationDataBkg = particleValidationData[validationLabels[:,1].astype(bool)]\n",
    "particleTrainingLabelSig = trainingLabels[trainingLabels[:,0].astype(bool)]\n",
    "particleTrainingLabelBkg = trainingLabels[trainingLabels[:,1].astype(bool)]\n",
    "\n",
    "# Jet mass for correlation\n",
    "jetMassTrainingDataSig = jetMassTrainingData[trainingLabels[:,0].astype(bool)]\n",
    "jetMassTrainingDataBkg = jetMassTrainingData[trainingLabels[:,1].astype(bool)]\n",
    "jetMassValidationDataSig = jetMassValidationData[validationLabels[:,0].astype(bool)]\n",
    "jetMassValidationDataBkg = jetMassValidationData[validationLabels[:,1].astype(bool)]\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(jetMassTrainingDataSig,color=\"r\",bins=20,alpha=0.5,label=\"Z'\")\n",
    "ax.hist(jetMassTrainingDataBkg,color=\"b\",bins=20,alpha=0.5,label=\"QCD\")\n",
    "plt.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"Jet mass (GeV))\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4103a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the interaction matrices\n",
    "class GraphNetnoSV(nn.Module):\n",
    "    def __init__(self, n_constituents, n_targets, params, hidden, De=5, Do=6, softmax=False):\n",
    "        super(GraphNetnoSV, self).__init__()\n",
    "        self.hidden = int(hidden)\n",
    "        self.P = params\n",
    "        self.Nv = 0 \n",
    "        self.N = n_constituents\n",
    "        self.Nr = self.N * (self.N - 1)\n",
    "        self.Nt = self.N * self.Nv\n",
    "        self.Ns = self.Nv * (self.Nv - 1)\n",
    "        self.Dr = 0\n",
    "        self.De = De\n",
    "        self.Dx = 0\n",
    "        self.Do = Do\n",
    "        self.S = 0\n",
    "        self.n_targets = n_targets\n",
    "        self.assign_matrices()\n",
    "           \n",
    "        self.Ra = torch.ones(self.Dr, self.Nr)\n",
    "        self.fr1 = nn.Linear(2 * self.P + self.Dr, self.hidden).cuda()\n",
    "        self.fr2 = nn.Linear(self.hidden, int(self.hidden/2)).cuda()\n",
    "        self.fr3 = nn.Linear(int(self.hidden/2), self.De).cuda()\n",
    "        self.fr1_pv = nn.Linear(self.S + self.P + self.Dr, self.hidden).cuda()\n",
    "        self.fr2_pv = nn.Linear(self.hidden, int(self.hidden/2)).cuda()\n",
    "        self.fr3_pv = nn.Linear(int(self.hidden/2), self.De).cuda()\n",
    "        \n",
    "        self.fo1 = nn.Linear(self.P + self.Dx + (self.De), self.hidden).cuda()\n",
    "        self.fo2 = nn.Linear(self.hidden, int(self.hidden/2)).cuda()\n",
    "        self.fo3 = nn.Linear(int(self.hidden/2), self.Do).cuda()\n",
    "        \n",
    "        self.fc_fixed = nn.Linear(self.Do, self.n_targets).cuda()\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "            \n",
    "    def assign_matrices(self):\n",
    "        self.Rr = torch.zeros(self.N, self.Nr)\n",
    "        self.Rs = torch.zeros(self.N, self.Nr)\n",
    "        receiver_sender_list = [i for i in itertools.product(range(self.N), range(self.N)) if i[0]!=i[1]]\n",
    "        for i, (r, s) in enumerate(receiver_sender_list):\n",
    "            self.Rr[r, i] = 1\n",
    "            self.Rs[s, i] = 1\n",
    "        self.Rr = (self.Rr).cuda()\n",
    "        self.Rs = (self.Rs).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###PF Candidate - PF Candidate###\n",
    "        Orr = self.tmul(x, self.Rr)\n",
    "        Ors = self.tmul(x, self.Rs)\n",
    "        B = torch.cat([Orr, Ors], 1)\n",
    "        ### First MLP ###\n",
    "        B = torch.transpose(B, 1, 2).contiguous()\n",
    "        B = nn.functional.relu(self.fr1(B.view(-1, 2 * self.P + self.Dr)))\n",
    "        B = nn.functional.relu(self.fr2(B))\n",
    "        E = nn.functional.relu(self.fr3(B).view(-1, self.Nr, self.De))\n",
    "        del B\n",
    "        E = torch.transpose(E, 1, 2).contiguous()\n",
    "        Ebar_pp = self.tmul(E, torch.transpose(self.Rr, 0, 1).contiguous())\n",
    "        del E\n",
    "        \n",
    "\n",
    "        ####Final output matrix for particles###\n",
    "        \n",
    "\n",
    "        C = torch.cat([x, Ebar_pp], 1)\n",
    "        del Ebar_pp\n",
    "        C = torch.transpose(C, 1, 2).contiguous()\n",
    "        ### Second MLP ###\n",
    "        C = nn.functional.relu(self.fo1(C.view(-1, self.P + self.Dx + (self.De))))\n",
    "        C = nn.functional.relu(self.fo2(C))\n",
    "        O = nn.functional.relu(self.fo3(C).view(-1, self.N, self.Do))\n",
    "        del C\n",
    "\n",
    "        \n",
    "        #Taking the sum of over each particle/vertex\n",
    "        N = torch.sum(O, dim=1)\n",
    "        del O\n",
    "        \n",
    "        ### Classification MLP ###\n",
    "\n",
    "        N = self.fc_fixed(N)\n",
    "        \n",
    "        if softmax:\n",
    "            N = nn.Softmax(dim=1)(N)\n",
    "        \n",
    "        return self.activation(N)\n",
    "            \n",
    "    def tmul(self, x, y):  #Takes (I * J * K)(K * L) -> I * J * L \n",
    "        x_shape = x.size()\n",
    "        y_shape = y.size()\n",
    "        return torch.mm(x.view(-1, x_shape[2]), y).view(-1, x_shape[1], y_shape[1])\n",
    "    \n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, n_targets):\n",
    "        super(DNN, self).__init__()\n",
    "        #self.flat = torch.flatten()\n",
    "        self.f0 = nn.Linear(200, 400).cuda()\n",
    "        self.f0b = nn.Linear(400, 400).cuda()\n",
    "        self.f1 = nn.Linear(400, 100).cuda()\n",
    "        self.f2 = nn.Linear(100, 50).cuda()\n",
    "        self.f3 = nn.Linear(50, 10).cuda()\n",
    "        self.f4 = nn.Linear(10, n_targets).cuda()\n",
    "        self.activation = torch.nn.ReLU()\n",
    "    def forward(self, x): \n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.activation(self.f0(x))\n",
    "        #x = self.f0b(x)\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.f4(x)\n",
    "        return(x)\n",
    "    \n",
    "\n",
    "class simple_MLP(torch.nn.Module):\n",
    "    def __init__(self,input_size=5,out_channels=2,act_out=True,nlayers=4,nhidden=50,batchnorm=True):\n",
    "        super().__init__()\n",
    "        self.bn  = torch.nn.BatchNorm1d(input_size).cuda()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 50, bias=False).cuda()\n",
    "        self.ac1 = torch.nn.ReLU().cuda()\n",
    "        self.dp1 = torch.nn.Dropout(p=0.2).cuda()\n",
    "        self.fc2 = torch.nn.Linear(50, 30).cuda()\n",
    "        self.ac2 = torch.nn.ReLU().cuda()\n",
    "        self.fc3 = torch.nn.Linear(30, 10).cuda()\n",
    "        self.ac3 = torch.nn.ReLU().cuda()\n",
    "        self.fc4 = torch.nn.Linear(10, out_channels).cuda()\n",
    "        self.output = torch.nn.Sigmoid().cuda()\n",
    "        self.out_channels = out_channels\n",
    "        self.act_out = act_out\n",
    "        self.nlayers = nlayers\n",
    "        self.runbatchnorm = batchnorm\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.fc1(x)\n",
    "        #x = self.ac1(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.ac2(x)\n",
    "        x = self.fc3(x)\n",
    "        #x = self.ac3(x)\n",
    "        x = self.fc4(x)\n",
    "        #if self.runbatchnorm:\n",
    "        #    x = self.batchnorm(x)\n",
    "        #if self.act_out:\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define losses \n",
    "class BarlowTwinsLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, lambda_param=5e-3):\n",
    "        super(BarlowTwinsLoss, self).__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "        self.device = torch.device('cuda:0')\n",
    "\n",
    "    def forward(self, z_a: torch.Tensor, z_b: torch.Tensor):\n",
    "        #self.device = (torch.device('cuda')if z_a.is_cuda else torch.device('cpu'))\n",
    "        # normalize repr. along the batch dimension\n",
    "        z_a_norm = (z_a - z_a.mean(0)) / z_a.std(0) # NxD\n",
    "        z_b_norm = (z_b - z_b.mean(0)) / z_b.std(0) # NxD\n",
    "\n",
    "        N = z_a.size(0)\n",
    "        D = z_a.size(1)\n",
    "\n",
    "        # cross-correlation matrix\n",
    "        c = torch.mm(z_a_norm.T, z_b_norm) / N # DxD\n",
    "        # loss\n",
    "        c_diff = (c - torch.eye(D, device=self.device)).pow(2) # DxD\n",
    "        # multiply off-diagonal elems of c_diff by lambda\n",
    "        c_diff[~torch.eye(D, dtype=bool)] *= self.lambda_param\n",
    "        loss = c_diff.sum()\n",
    "        return loss\n",
    "\n",
    "class CorrLoss(nn.Module):\n",
    "    def __init__(self, corr=False,sort_tolerance=1.0,sort_reg='l2'):\n",
    "        super(CorrLoss, self).__init__()\n",
    "        self.tolerance = sort_tolerance\n",
    "        self.reg       = sort_reg\n",
    "        self.corr      = corr\n",
    "        \n",
    "    def spearman(self, pred, target):\n",
    "        pred   = soft_rank(pred.cpu().reshape(1,-1),regularization=self.reg,regularization_strength=self.tolerance,)\n",
    "        target = soft_rank(target.cpu().reshape(1,-1),regularization=self.reg,regularization_strength=self.tolerance,)\n",
    "        #pred   = torchsort.soft_rank(pred.reshape(1,-1),regularization_strength=x)\n",
    "        #target = torchsort.soft_rank(target.reshape(1,-1),regularization_strength=x)\n",
    "        pred = pred - pred.mean()\n",
    "        pred = pred / pred.norm()\n",
    "        target = target - target.mean()\n",
    "        target = target / target.norm()\n",
    "        ret = (pred * target).sum()\n",
    "        if self.corr:\n",
    "            return (1-ret)*(1-ret)\n",
    "        else:\n",
    "            return ret*ret \n",
    "    \n",
    "    def forward(self, features, labels):\n",
    "        return self.spearman(features,labels)\n",
    "    \n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "class VICRegLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, lambda_param=1,mu_param=1,nu_param=20):\n",
    "        super(VICRegLoss, self).__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "        self.mu_param = mu_param\n",
    "        self.nu_param = nu_param\n",
    "        #self.device = torch.device('cpu')\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.device = (torch.device('cuda')if x.is_cuda else torch.device('cpu'))\n",
    "        \n",
    "        x_scale = x\n",
    "        y_scale = y\n",
    "        repr_loss = F.mse_loss(x_scale, y_scale)\n",
    "        \n",
    "        #x = torch.cat(FullGatherLayer.apply(x), dim=0)\n",
    "        #y = torch.cat(FullGatherLayer.apply(y), dim=0)\n",
    "        x = x_scale - x_scale.mean(dim=0)\n",
    "        y = y_scale - y_scale.mean(dim=0)\n",
    "        N = x_scale.size(0)\n",
    "        D = x_scale.size(1)\n",
    "        \n",
    "        std_x = torch.sqrt(x_scale.var(dim=0) + 0.0001)\n",
    "        std_y = torch.sqrt(y_scale.var(dim=0) + 0.0001)\n",
    "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "        cov_x = (x_scale.T @ x_scale) / (N - 1)\n",
    "        cov_y = (y_scale.T @ y_scale) / (N - 1)\n",
    "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(D) + off_diagonal(cov_y).pow_(2).sum().div(D)\n",
    "\n",
    "        #loss = (self.lambda_param * repr_loss + self.mu_param * std_loss+ self.nu_param * cov_loss)\n",
    "        #print(repr_loss,cov_loss,std_loss)\n",
    "        return repr_loss,cov_loss,std_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c614dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Training Loop Barlow DNN #########\n",
    "\n",
    "batchSize = 6000\n",
    "n_epochs = 10\n",
    "\n",
    "gnn = DNN(n_encoded_nodes)\n",
    "    \n",
    "loss = nn.BCELoss(reduction='mean')\n",
    "clr_criterion  = VICRegLoss(lambda_param=1.0)\n",
    "cor_criterion  = CorrLoss()\n",
    "acr_criterion  = CorrLoss(corr=True)\n",
    "\n",
    "BarlowLoss = True\n",
    "\n",
    "optimizer = optim.Adam(gnn.parameters(), lr = 0.0001)\n",
    "loss_vals_training = np.zeros(n_epochs)\n",
    "loss_std_training = np.zeros(n_epochs)\n",
    "loss_vals_validation = np.zeros(n_epochs)\n",
    "loss_std_validation = np.zeros(n_epochs)\n",
    "\n",
    "acc_vals_training = np.zeros(n_epochs)\n",
    "acc_vals_validation = np.zeros(n_epochs)\n",
    "acc_std_training = np.zeros(n_epochs)\n",
    "acc_std_validation = np.zeros(n_epochs)\n",
    "\n",
    "final_epoch = 0\n",
    "l_val_best = 99999\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "for m in range(n_epochs):\n",
    "    print(\"Epoch %s\\n\" % m)\n",
    "    torch.cuda.empty_cache()\n",
    "    final_epoch = m\n",
    "    lst = []\n",
    "    loss_val = []\n",
    "    loss_training = []\n",
    "    correct = []\n",
    "    tic = time.perf_counter()\n",
    "    \n",
    "    particleTrainingDataSig, jetMassTrainingDataSig = sklearn.utils.shuffle(particleTrainingDataSig, jetMassTrainingDataSig)\n",
    "    particleTrainingDataBkg, jetMassTrainingDataBkg = sklearn.utils.shuffle(particleTrainingDataBkg, jetMassTrainingDataBkg)\n",
    "    particleValidationDataSig, jetMassValidationDataSig = sklearn.utils.shuffle(particleValidationDataSig,\n",
    "                                                                                jetMassValidationDataSig)\n",
    "    particleValidationDataBkg, jetMassValidationDataBkg = sklearn.utils.shuffle(particleValidationDataBkg,\n",
    "                                                                                jetMassValidationDataBkg)\n",
    "    \n",
    "\n",
    "    out1_totSig = np.empty((0,n_encoded_nodes))\n",
    "    out1_totBkg = np.empty((0,n_encoded_nodes))\n",
    "\n",
    "    trainingv1_mass_totSig = []\n",
    "    trainingv1_mass_totBkg = []\n",
    "\n",
    "    #print(out1_tot)\n",
    "    for i in tqdm(range(int(0.8*datapoints/batchSize))): \n",
    "        #print('%s out of %s'%(i, int(particleTrainingData.shape[0]/batchSize)))\n",
    "        optimizer.zero_grad()\n",
    "        trainingvSig = torch.Tensor(particleTrainingDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvBkg = torch.Tensor(particleTrainingDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassSig = torch.Tensor(jetMassTrainingDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassBkg = torch.Tensor(jetMassTrainingDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingv1 = torch.cat((trainingvSig[:int(batchSize/2)], \n",
    "                                trainingvBkg[:int(batchSize/2)]))\n",
    "        trainingv1_mass = torch.cat((trainingvMassSig[:int(batchSize/2)], \n",
    "                                trainingvMassBkg[:int(batchSize/2)]))\n",
    "        trainingv2 = torch.cat((trainingvSig[int(batchSize/2):], \n",
    "                                trainingvBkg[int(batchSize/2):]))\n",
    "        trainingv2_mass = torch.cat((trainingvMassSig[int(batchSize/2):], \n",
    "                                trainingvMassBkg[int(batchSize/2):]))\n",
    "        # Calculate network output\n",
    "        out1 = gnn(trainingv1)\n",
    "        out2 = gnn(trainingv2)\n",
    "        \n",
    "        # VICReg Loss\n",
    "        repr_loss, cov_loss, std_loss = weightClr*clr_criterion(out1, out2)\n",
    "        l = weightrepr*repr_loss + weightcov*cov_loss + weightstd*std_loss\n",
    "        #l = repr_loss + cov_loss + 10*std_loss\n",
    "        # AntiCorrelation\n",
    "        l += weightCorr1*acr_criterion(trainingv1_mass, out1[:,0])\n",
    "        l += weightCorr1*acr_criterion(trainingv2_mass, out2[:,0])\n",
    "       \n",
    "        # Correlation for rest of dimensions\n",
    "        for dim in range(1,n_encoded_nodes): \n",
    "            l += weightCorr2*cor_criterion(out1[:,dim], trainingv1_mass)\n",
    "            l += weightCorr2*cor_criterion(out2[:,dim], trainingv2_mass)\n",
    "                    \n",
    "        # Classical BCE loss\n",
    "        #trainingv = torch.FloatTensor(particleTrainingData[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        #out = gnn(trainingv)\n",
    "        #targetv = torch.FloatTensor(trainingLabels[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        #l = loss(out, targetv)\n",
    "        \n",
    "        \n",
    "        loss_training.append(l.item())\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loss_string = \"Loss: %s\" % \"{0:.5f}\".format(l.item())\n",
    "        out1 = out1.cpu().detach().numpy()\n",
    "        out1_totSig = np.concatenate((out1_totSig,out1[:int(batchSize/2)]))\n",
    "        out1_totBkg = np.concatenate((out1_totBkg,out1[int(batchSize/2):]))\n",
    "        out2 = out2.cpu().detach().numpy()\n",
    "        out1_totSig = np.concatenate((out1_totSig,out2[:int(batchSize/2)]))\n",
    "        out1_totBkg = np.concatenate((out1_totBkg,out2[int(batchSize/2):]))\n",
    "\n",
    "        \n",
    "        trainingv1_mass = trainingv1_mass.cpu().detach().numpy().tolist()\n",
    "        trainingv1_mass_totSig += trainingv1_mass[:int(batchSize/2)]\n",
    "        trainingv1_mass_totBkg += trainingv1_mass[int(batchSize/2):]\n",
    "\n",
    "        trainingv2_mass = trainingv2_mass.cpu().detach().numpy().tolist()\n",
    "        trainingv1_mass_totSig += trainingv2_mass[:int(batchSize/2)]\n",
    "        trainingv1_mass_totBkg += trainingv2_mass[int(batchSize/2):]\n",
    "\n",
    "        \n",
    "        #print(\"SIG:\", trainingv1_mass_totSig)\n",
    "        #print(\"BKG:\", trainingv1_mass_totBkg)\n",
    "\n",
    "        del trainingvSig, trainingvBkg, l, trainingv1, trainingv2#, out1, out2, trainingv1_mass\n",
    "        torch.cuda.empty_cache()\n",
    "    #trainingv1_mass_tot = np.array(trainingv1_mass_tot)       \n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Training done in {toc - tic:0.4f} seconds\")\n",
    "    tic = time.perf_counter()\n",
    "\n",
    "    \n",
    "    plt.clf()\n",
    "    fig, axs = plt.subplots(n_encoded_nodes,2, figsize=(10,dim*10))\n",
    "    \n",
    "    axs[0,0].text(0.05,2.8, loss_text, transform=ax.transAxes)\n",
    "\n",
    "    for dim in range(out1.shape[1]): \n",
    "        outSig, massSig = out1_totSig[:, dim].copy(), trainingv1_mass_totSig[:].copy()\n",
    "        outSig -= np.mean(outSig)\n",
    "        outSig /= np.std(outSig)\n",
    "        massSig -= np.mean(massSig)\n",
    "        massSig /= np.std(massSig)\n",
    "\n",
    "        outBkg, massBkg = out1_totBkg[:, dim].copy(), trainingv1_mass_totBkg[:].copy()\n",
    "        outBkg -= np.mean(outBkg)\n",
    "        outBkg /= np.std(outBkg)\n",
    "        massBkg -= np.mean(massBkg)\n",
    "        massBkg /= np.std(massBkg)\n",
    "\n",
    "        \n",
    "        axs[dim,0].text(0.8,1.03,f\"Z' Corr:  {np.corrcoef(outSig, massSig)[0,1] : .4f}\", transform=axs[dim,0].transAxes)\n",
    "        axs[dim,0].hist2d(outSig, trainingv1_mass_totSig[:], bins=30, )\n",
    "        axs[dim,1].text(0.8,1.03,f\"QCD Corr: {np.corrcoef(outBkg, massBkg)[0,1] : .4f}\", transform=axs[dim,1].transAxes)\n",
    "        axs[dim,1].hist2d(outBkg, trainingv1_mass_totBkg[:], bins=30, )\n",
    "        axs[dim,0].set_xlim([-3.,3.])\n",
    "        axs[dim,1].set_xlim([-3.,3.])\n",
    "        axs[dim,0].set_xlabel(f'Dimension {dim} output')\n",
    "        axs[dim,1].set_xlabel(f'Dimension {dim} output')\n",
    "        axs[dim,0].set_ylabel('Jet mass (GeV)')\n",
    "        \n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(outdir+\"/\"+label+f\"_contrastivefigIN_trainingDataset_epoch{m}.jpg\")\n",
    "    del out1, out2, trainingv1_mass\n",
    "\n",
    "    \n",
    "    for i in range(int(0.1*datapoints/(batchSize))): \n",
    "        torch.cuda.empty_cache()\n",
    "        trainingvSig_val = torch.Tensor(particleValidationDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvBkg_val = torch.Tensor(particleValidationDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassSig_val = torch.Tensor(jetMassValidationDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassBkg_val = torch.Tensor(jetMassValidationDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        targetv_val = torch.Tensor(validationLabels[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingv1_val = torch.cat((trainingvSig_val[:int(batchSize/2)], trainingvBkg_val[:int(batchSize/2)]))\n",
    "        trainingv2_val = torch.cat((trainingvSig_val[int(batchSize/2):], trainingvBkg_val[int(batchSize/2):]))\n",
    "        trainingv1_val_mass = torch.cat((trainingvMassSig_val[:int(batchSize/2)], \n",
    "                                trainingvMassBkg_val[:int(batchSize/2)]))\n",
    "        trainingv2_val_mass = torch.cat((trainingvMassSig_val[int(batchSize/2):], \n",
    "                                trainingvMassBkg_val[int(batchSize/2):]))\n",
    "\n",
    "        # Barlow Loss\n",
    "        out1_val = gnn(trainingv1_val)\n",
    "        out2_val = gnn(trainingv2_val)\n",
    "        \n",
    "        # VICReg Loss\n",
    "        repr_loss, cov_loss, std_loss = weightClr*clr_criterion(out1_val, out2_val)\n",
    "        l_val = repr_loss + cov_loss + 10*std_loss\n",
    "        \n",
    "        # AntiCorrelation\n",
    "        l_val += weightCorr1*acr_criterion(trainingv1_val_mass, out1_val[:,0])\n",
    "        l_val += weightCorr1*acr_criterion(trainingv2_val_mass, out2_val[:,0])\n",
    "        \n",
    "        # Correlation for rest of dimensions\n",
    "        for dim in range(1,n_encoded_nodes): \n",
    "            l_val += weightCorr2*cor_criterion(out1_val[:,dim], trainingv1_val_mass)\n",
    "            l_val += weightCorr2*cor_criterion(out2_val[:,dim], trainingv2_val_mass)\n",
    "        \n",
    "        # Classical validation\n",
    "        trainingv_val = torch.Tensor(particleValidationData[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        out = gnn(trainingv_val)\n",
    "        # l_val = loss(out, targetv_val)\n",
    "        lst.append(out.cpu().data.numpy())\n",
    "        loss_val.append(l_val.item())\n",
    "        correct.append(targetv_val.cpu())\n",
    "        out1_val = out1_val.cpu().detach().numpy()\n",
    "        trainingv1_val_mass = trainingv1_val_mass.cpu().detach().numpy()\n",
    "        \n",
    "        \n",
    "        del trainingvSig_val, trainingvBkg_val, targetv_val, trainingv1_val, trainingv2_val,out2_val\n",
    "    torch.cuda.empty_cache()\n",
    "    plt.clf()\n",
    "    fig, axs = plt.subplots(n_encoded_nodes, figsize=(10,50))\n",
    "    for dim in range(out1_val.shape[1]): \n",
    "        axs[dim].plot(out1_val[:int(batchSize/2), dim], trainingv1_val_mass[:int(batchSize/2)], 'k+',c='r', alpha=0.5)\n",
    "        #plt.xlabel('%s dimension output'%(dim))\n",
    "        axs[dim].plot(out1_val[int(batchSize/2):, dim], trainingv1_val_mass[int(batchSize/2):], 'k+',c='b', alpha=0.5)\n",
    "\n",
    "        axs[dim].set_ylabel('sdmass')\n",
    "    plt.savefig(outdir+\"/\"+label+\"_contrastivefigIN_validationDataset.jpg\")\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    fig = corner.corner(out1_val[:int(batchSize/2)], color='red')\n",
    "    corner.corner(out1_val[int(batchSize/2):], fig=fig, color='blue')\n",
    "    plt.savefig(outdir+\"/\"+label+\"corner.jpg\")\n",
    "    plt.clf()\n",
    "    del out1_val, trainingv1_val_mass\n",
    "    #targetv_cpu = targetv.cpu().data.numpy()\n",
    "    \n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Evaluation done in {toc - tic:0.4f} seconds\")\n",
    "    l_val = np.mean(np.array(loss_val))\n",
    "\n",
    "    predicted = np.concatenate(lst) #(torch.FloatTensor(np.concatenate(lst))).to(device)\n",
    "    print('\\nValidation Loss: ', l_val)\n",
    "\n",
    "    l_training = np.mean(np.array(loss_training))\n",
    "    print('Training Loss: ', l_training)\n",
    "    val_targetv = np.concatenate(correct) #torch.FloatTensor(np.array(correct)).cuda()\n",
    "\n",
    "    torch.save(gnn.state_dict(), '%s/DNN_%s_last.pth'%(outdir,label))\n",
    "    if l_val < l_val_best:\n",
    "        print(\"new best model\")\n",
    "        l_val_best = l_val\n",
    "        torch.save(gnn.state_dict(), '%s/DNN_%s_best.pth'%(outdir,label))\n",
    "\n",
    "    print(val_targetv.shape, predicted.shape)\n",
    "    print(val_targetv, predicted)\n",
    "    acc_vals_validation[m] = accuracy_score(val_targetv[:,0],predicted[:,0]>0.5)\n",
    "    print(\"Validation Accuracy: \", acc_vals_validation[m])\n",
    "    loss_vals_training[m] = l_training\n",
    "    loss_vals_validation[m] = l_val\n",
    "    loss_std_validation[m] = np.std(np.array(loss_val))\n",
    "    loss_std_training[m] = np.std(np.array(loss_training))\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if m > 8 and all(loss_vals_validation[max(0, m - 8):m] > min(np.append(loss_vals_validation[0:max(0, m - 8)], 200))):\n",
    "        print('Early Stopping...')\n",
    "        print(loss_vals_training, '\\n', np.diff(loss_vals_training))\n",
    "        break\n",
    "    torch.cuda.empty_cache()\n",
    "print('DONE with DNN training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b37a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "\n",
    "#r=torch.randperm(len(trainingLabels))\n",
    "x_torch=torch.load(f\"{outdir}/{label}_particleTrainingData.pt\").cuda()\n",
    "y_torch=torch.load(f\"{outdir}/{label}_trainingLabels.pt\").cuda()\n",
    "m_torch=torch.load(f\"{outdir}/{label}_jetMassTrainingData.pt\").cuda()\n",
    "\n",
    "gnn = DNN(n_encoded_nodes)\n",
    "gnn.load_state_dict(torch.load('%s/DNN_%s_best.pth'%(outdir,label), map_location=torch.device('cuda')))\n",
    "gnn.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_optimal_clr = gnn(x_torch)\n",
    "\n",
    "del x_torch\n",
    "torch.cuda.empty_cache()\n",
    "simple_model = simple_MLP(input_size=n_encoded_nodes-1)\n",
    "simple_optimizer = torch.optim.Adam(simple_model.parameters(), lr=.1) \n",
    "simple_criterion = torch.nn.BCELoss()\n",
    "\n",
    "for epoch in range(51):\n",
    "    simple_optimizer.zero_grad()\n",
    "    outputs = simple_model(outputs_optimal_clr[:,1:])\n",
    "    loss = simple_criterion(outputs,y_torch)\n",
    "    loss.backward()\n",
    "    simple_optimizer.step()\n",
    "    current_loss = float(loss.item())\n",
    "    if epoch % 10 == 1: \n",
    "        print(f\"Epoch {epoch} loss={current_loss:.5f}\")\n",
    "\n",
    "print(simple_model.state_dict())\n",
    "del simple_optimizer, loss, simple_model, simple_criterion\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69053458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "\n",
    "fig, ax=plt.subplots()\n",
    "ax.hist(outputs[y_torch[:,1]==0][:,1].cpu().detach().numpy(),color=\"r\",bins=np.linspace(0.,1.,25),alpha=0.5,label=\"QCD\")\n",
    "ax.hist(outputs[y_torch[:,1]==1][:,1].cpu().detach().numpy(),color=\"b\",bins=np.linspace(0.,1.,25),alpha=0.5,label=\"Z'\")\n",
    "plt.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"Discriminator\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "ax.text(0.45,1.03,loss_text, transform=ax.transAxes)\n",
    "plt.savefig(outdir+\"/discriminatorHist.png\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax=plt.subplots()\n",
    "#print(outputs[:,0].cpu().detach().numpy(),y_torch[:,0].cpu().detach().numpy().astype(bool))\n",
    "fpr, tpr, _ = roc_curve(y_torch[:,0].cpu().detach().numpy(),outputs[:,0].cpu().detach().numpy(),)\n",
    "ax.plot(fpr, tpr)\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.text(0.45,1.03,loss_text, transform=ax.transAxes)\n",
    "ax.text(0.75,0.10, f\"AUC={roc_auc_score(y_torch[:,0].cpu().detach().numpy(),outputs[:,0].cpu().detach().numpy()) : 0.2f}\", transform=ax.transAxes)\n",
    "plt.savefig(outdir+\"/roc.png\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# Calculate mass distribution after cuts\n",
    "\n",
    "hist, edges = np.histogram(outputs[y_torch[:,1].cpu().detach().numpy()==1][:,1].cpu().detach().numpy(), bins=np.linspace(0.,1.,100),density=True)\n",
    "cdf = np.cumsum(hist)*(edges[1]-edges[0])\n",
    "print(cdf)\n",
    "\n",
    "pctls = [0.,0.25,0.5,0.7,0.9,0.95,0.99]\n",
    "cuts = np.searchsorted(cdf,pctls)\n",
    "\n",
    "fig, ax=plt.subplots()\n",
    "\n",
    "qcd_idxs = y_torch[:,1].cpu().detach().numpy()==1\n",
    "qcd_inclusive, _ = np.histogram(\n",
    "        m_torch[(qcd_idxs)].cpu().detach().numpy(),\n",
    "        density=True,\n",
    "    )\n",
    "for c,p in zip(cuts,pctls):\n",
    "    passing_idxs = outputs[:,1].cpu().detach().numpy() > edges[c]\n",
    "    hist, bin_edges = np.histogram(\n",
    "        m_torch[(qcd_idxs&passing_idxs)].cpu().detach().numpy(), \n",
    "    )\n",
    "    N_passing = float(np.sum(hist))\n",
    "    qcd_passing = np.divide(hist,[N_passing])\n",
    "    jsd = scipy.spatial.distance.jensenshannon(qcd_passing, qcd_inclusive)\n",
    "\n",
    "    bins_centers = 0.5*(bin_edges[1:]+bin_edges[:-1])\n",
    "    ax.plot(\n",
    "        bins_centers, \n",
    "        qcd_passing,\n",
    "        label = f\"{(1-p)*100:.0f}% ({int(N_passing)}) JSD={0 if jsd == np.nan else jsd:.2f}\"\n",
    "    )\n",
    "ax.set_xlabel(\"Jet mass (GeV)\")\n",
    "ax.set_ylabel(\"a.u.\")\n",
    "plt.legend(loc=\"best\")\n",
    "ax.text(0.05,1.03,\"QCD jets\", transform=ax.transAxes)\n",
    "ax.text(0.45,1.03,loss_text, transform=ax.transAxes)\n",
    "plt.savefig(outdir+\"/sculptingQCD.png\")\n",
    "plt.show()\n",
    "print(cuts)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax=plt.subplots()\n",
    "ax.hist2d(outputs[y_torch[:,1]==1][:,0].cpu().detach().numpy(),m_torch[y_torch[:,1]==1].cpu().detach().numpy(),bins=20)\n",
    "#ax.plot(m_torch.cpu().detach().numpy(),outputs[:,1].cpu().detach().numpy(),color=\"b\",label=\"Z'\")\n",
    "plt.legend(loc=\"best\")\n",
    "ax.set_ylabel(\"Jet mass (GeV)\")\n",
    "ax.set_xlabel(\"Discriminator\")\n",
    "ax.text(0.05,1.03,\"QCD jets\", transform=ax.transAxes)\n",
    "ax.text(0.45,1.03,loss_text, transform=ax.transAxes)\n",
    "plt.savefig(outdir+\"/discriminatorvsMassQCD.png\")\n",
    "plt.show()\n",
    "\n",
    "del y_torch, m_torch, outputs, outputs_optimal_clr\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d5a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.system(f\"rm {outdir}/{label}_particleTrainingData.pt\")\n",
    "#os.system(f\"rm {outdir}/{label}_jetMassTrainingData.pt\")\n",
    "#os.system(f\"rm {outdir}/{label}_trainingLabels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6108ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
